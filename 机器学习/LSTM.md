## LSTM

RNN在处理long term memory的时候存在缺陷，因此LSTM应运而生。LSTM是一种变种的RNN，它的精髓在于引入了细胞状态这样一个概念，不同于RNN只考虑最近的状态，LSTM的细胞状态会决定哪些状态应该被留下来，哪些状态应该被遗忘

不管是RNN还是LSTM及其衍生主要是随着时间推移进行顺序处理
水平箭头的意思是长期信息需在进入当前处理单元前顺序遍历所有单元。这意味着其能轻易被乘以很多次<0的小数而损坏。这是导致vanishing gradients（梯度消失）问题的原因。
为此，今天被视为救星的LSTM模型出现了，有点像ResNet模型，可以绕过单元从而记住更长的时间步骤（LSTM之所可以解决梯度问题是因为它避免了无休止的连乘，而是边加边乘，这和resnet本身就是异曲同工的）。因此，LSTM可以消除一些梯度消失的问题。
缺点总结
（1）RNN的梯度问题在LSTM及其变种里面得到了一定程度的解决，但还是不够。它可以处理100个量级的序列，而对于1000个量级，或者更长的序列则依然会显得很棘手。
（2）计算费时。每一个LSTM的cell里面都意味着有4个全连接层(MLP),如果LSTM的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。

![lda](../Images/machinelearning/lstm.png)

GRU和LSTM的性能在很多任务上不分伯仲。
GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。
从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。



### 梯度消失和梯度爆炸

通常来说当激活函数是sigmoid时,梯度消失比梯度爆炸更容易发生

层数比较多的神经网络模型在使用梯度下降法对误差进行反向传播时会出现梯度消失和梯度爆炸问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显。
例如，对于图1所示的含有3个隐藏层的神经网络，梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题。

如何解决
- 用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。
- 用Batch Normalization。
- LSTM的结构设计也可以改善RNN中的梯度消失问题。

### 参考资料

https://www.jianshu.com/p/9dc9f41f0b29