## 机器学习问题

- **为什么需要对数值类型的特征做归一化？**
为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得 不同指标之间具有可比性。
（1）线性函数归一化（Min-Max Scaling）
（2）零均值归一化（Z-Score Normalization）
在实际应用中，通过梯度下降法求解的模 型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模 型。
但对于决策树模型则并不适用，以C4.5为例，决策树在进行节点分裂时主要 依据数据集D关于特征x的信息增益比（详见第3章第3节），而信息增益比跟特征 是否经过归一化是无关的，因为归一化并不会改变样本在特征x上的信息增益。

- **有哪些文本表示模型？它们各有什么优缺点？**
1. 词袋模型和N-gram模型
向量中的每一维代表一个单词，而该维对 应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重
2. 主题模型
主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布 特性），并且能够计算出每篇文章的主题分布
3. 词嵌入(word embeding)与深度学习模型
词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维 空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一 维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

- **Word2Vec是如何工作的？它和LDA有什么区别与联系？**
Word2Vec实际 是一种浅层的神经网络模型，它有两种网络结构，分别是CBOW（Continues Bag of Words）和Skip-gram。
接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成 概率最大化。从输入层到隐含层需要一个维度为N×K的权重矩阵，从隐含层到输 出层又需要一个维度为K×N的权重矩阵，学习权重可以用反向传播算法实现，每 次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于Softmax激活函数中 存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍 历，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和Negative Sampling两种改进方法

LDA是利用文档中单词的共现关 系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行 学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了 上下文共现的特征。
主题模型和词 嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测 的隐含变量（即主题）；而词嵌入模型一般表达为神经网络的形式，似然函数定 义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。



 **在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带 来的问题?**
训练数据不足带来的问题主要表现在过拟合方面
处理方法大致也可以分两类，一是基于模型的方法，主要是采用降 低过拟合风险的措施，包括简化模型（如将非线性模型简化为线性模型）、添加 约束项以缩小假设空间（如L1/L2正则项）、集成学习、Dropout超参数等；二是基 于数据的方法，主要通过数据扩充（Data Augmentation），即根据一些先验知 识，在保持特定信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果



 **什么是ROC曲线？**
ROC曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性 率（True Positive Rate，TPR）
ROC曲线是通过不断移动分类器的“截断点”来生成曲线上的一组关 键点的, 上面所说的“截断点”指 的就是区分正负预测结果的阈值。
AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。
ROC曲线有一个特点，当正负样本的分布发生变化时，ROC曲线的形状 能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。
ROC曲线的适用场景更多，被广泛 用于排序、推荐、广告等领域。



 **在模型评估过程中，有哪些主要的验证方法，它们的优缺点是什么?**
Holdout检验
Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分 成训练集和验证集两部分。
交叉检验
k-fold交叉验证：首先将全部样本划分成k个大小相等的样本子集；依次遍历 这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的 训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验 中，k经常取10。
自助法（Bootstrap）
自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行n次有 放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采 样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验 证，这就是自助法的验证过程



 **降低“过拟合”风险的方法**
（1）从数据入手，获得更多的训练数据
（2）降低模型复杂度
- 可用人工选择要保留的特征
- 模型选择算法
（3）正则化方法
- L1 regularization
  - 当w为正时，sgn(w)>0, 则更新后的w变小。
	  当w为负时，sgn(w)>0, 则更新后的w变大—因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。
- L2 regularization
	在不使用L2正则化时。求导结果中 w 前系数为 1，经变化后w前面系数为 1−ηλ/n ，由于η、λ、n都是正的。所以 1−ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来。

- L1正则化对所有参数的惩罚力度都一样，可以让一部分权重变为零，因此产生稀疏模型，能够去除某些特征（权重为0则等效于去除）
- L2正则化减少了权重的固定比例，使权重平滑。L2正则化不会使权重变为0（不会产生稀疏模型），所以选择了更多的特征
相同点：都用于避免过拟合
不同点：L1可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况。 L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况
（4）集成学习方法
   (5) Dropout超参数, Early stopping提前终止训练

### 决策树

ID3—— 最大信息增益
C4.5——最大信息增益比
CART——最大基尼指数（Gini）： CART是一颗二叉树，采用二元切割法，每一步将数据 按特征A的取值切成两份，分别进入左右子树

ID3是采用信息增益作为评价标准， 会倾向于取值较多的特征。
其次，从样本类型的角度，ID3只能处理离散型变量，而C4.5和CART都可以 处理连续型变量。
从应用角度，ID3和C4.5只能用于分类任务，而CART（Classification and Regression Tree，分类回归树）从名字就可以看出其不仅可以用于分类，也可以应 用于回归任务（回归树使用最小平方误差准则）。
从实现细节、优化过程等角度，这三种决策树还有一些不同。比如， ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处 理；ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复 用，而CART每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特 征可以被重复使用；ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART 直接利用全部数据发现所有可能的树结构进行对比。

预剪枝： 即在生成决策树的过程中提前停止树的增长。而后剪枝，是在已生 成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。
（1）当树到达一定深度的时候，停止树的生长。
（2）当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
（3）计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继 续扩展。
后剪枝：相比于预剪枝，后剪枝 方法通常可以得到泛化能力更强的决策树，但时间开销会更大。常见的后剪枝方法包括错误率降低剪枝（Reduced Error Pruning，REP）、悲 观剪枝（Pessimistic Error Pruning，PEP）、代价复杂度剪枝（Cost Complexity Pruning，CCP）、最小误差剪枝（Minimum Error Pruning，MEP）、CVP（Critical Value Pruning）、OPP（Optimal Pruning）等方法，这些剪枝方法各有利弊，关注 不同的优化角度，本文选取著名的CART剪枝方法CCP进行介绍。
（1）从完整决策树T0 开始，生成一个子树序列{T0 ,T1 ,T2 ,...,Tn }，其中Ti+1 由Ti 生 成，Tn 为树的根结点。
（2）在子树序列中，根据真实误差选择最佳的决策树。

![decision tree](../Images/machinelearning/decision_tree.png)